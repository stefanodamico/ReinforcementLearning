import gym
import numpy as np

import keras.models
from keras.models import load_model, Sequential
from keras.layers import Activation, Dense
from keras.optimizers import SGD, RMSprop, Adam, Adamax

import os
from pathlib import Path
import random
import shutil
import math

from collections import deque
from pyglet.window import key
import time

import matplotlib.pyplot as plt

# SETTINGS
bool_quit = False

# Define the Environments
env = gym.make('CarRacing-v0').env

# Number of Dimensions in the Observable Space and number of Control Actions in the Environments
print('Observation Space:', env.observation_space)
print('Action Space:', env.action_space)
print("\n")
print("Observation Space Param: 96x96x3 values for Red, Green and Blue pixels")
print("Observation Space Highs:", np.mean(env.observation_space.high))
print("Observation Space Lows:   ", np.mean(env.observation_space.low))
print("\n")

#  Lo stato è composto da 96x96 pixel (il colore è irrilevante), dall'immagine originale ritagliamo la traccia a 10x10 pixel.
import cv2


# libreria per l'elaborazione manuale dell'immagine da estrarre:

def transform(state):
    # La black_bar in basso è la sezione dello schermo con, da sinistra a destra:
    # - velocità,
    # - quattro sensori ABS,
    # - posizione del volante,
    # - giroscopio.
    bottom_black_bar = state[84:, 12:]  # This is the section of the screen that contains the bar
    # color is irrelavent, we grayscale it
    img = cv2.cvtColor(bottom_black_bar, cv2.COLOR_RGB2GRAY)
    # La funzione cv.threshold viene utilizzata per applicare la soglia.
    # Il primo argomento è l'immagine sorgente, che dovrebbe essere un'immagine in scala di grigi.
    # Il secondo argomento è il valore di soglia che viene utilizzato per classificare i valori dei pixel.
    # Il terzo argomento è il valore massimo che viene assegnato ai valori dei pixel che superano la soglia.
    bottom_black_bar_bw = cv2.threshold(img, 1, 255, cv2.THRESH_BINARY)[1]
    # La funzione cv.resize viene utilizzata per ridimensionare l'immagine
    # 84 è il fattore di scala lungo l'asse X / 12 è il fattore di scala lungo l'asse Y
    bottom_black_bar_bw = cv2.resize(bottom_black_bar_bw, (84, 12), interpolation=cv2.INTER_NEAREST)

    # upper_field = observation[:84, :96] # this is the section of the screen that contains the track.
    upper_field = state[:84, 6:90]  # This is the section of the screen that contains the track
    # color is irrelavent, we grayscale it
    img = cv2.cvtColor(upper_field, cv2.COLOR_RGB2GRAY)
    #  Il primo argomento è l'immagine sorgente, che dovrebbe essere un'immagine in scala di grigi.
    #  Il secondo argomento è il valore di soglia che viene utilizzato per classificare i valori dei pixel.
    #  Il terzo argomento è il valore massimo che viene assegnato ai valori dei pixel che superano la soglia
    upper_field_bw = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY)[1]
    # rescaled to 10*10 pixels
    upper_field_bw = cv2.resize(upper_field_bw, (10, 10), interpolation=cv2.INTER_NEAREST)
    upper_field_bw = upper_field_bw.astype('float') / 255

    car_field = state[66:78, 43:53]  # This is the section of the screen that contains the track
    # color is irrelavent, we grayscale it
    img = cv2.cvtColor(car_field, cv2.COLOR_RGB2GRAY)
    car_field_bw = cv2.threshold(img, 80, 255, cv2.THRESH_BINARY)[1]
    # .mean() Restituisce la media degli elementi dell'array.
    car_field_t = [
        car_field_bw[:, 3].mean() / 255,
        car_field_bw[:, 4].mean() / 255,
        car_field_bw[:, 5].mean() / 255,
        car_field_bw[:, 6].mean() / 255]

    return bottom_black_bar_bw, upper_field_bw, car_field_t


def convert_argmax_qval_to_env_action(output_value):
    # Riduciamo lo spazio delle azione a 11 valori.
    # 9 per lo sterzo, 2 per il gas/freno.
    # per ridurre lo spazio di azione, gas e freno non possono essere applicati contemporaneamente.
    # anche sterzare e il accelerare/frenare non possono essere applicati contemporaneamente.
    # in modo simile alla guida reale, si frena/accelera in linea retta, si curva mentre si sterza.

    gaz = 0.0
    brake = 0.0
    steering = 0.0

    # output value ranges from 0 to 10
    if output_value <= 8:
        # steering. brake and gas are zero.
        output_value -= 4
        steering = float(output_value) / 4
    elif output_value >= 9 and output_value <= 9:
        output_value -= 8
        gaz = float(output_value) / 3  # 33% gas
    elif output_value >= 10 and output_value <= 10:
        output_value -= 9
        brake = float(output_value) / 2  # 50% brakes
    else:
        print("[WARNING] Error in convert_argmax_qval_to_env_action()")

    white = np.ones((round(brake * 100), 10))
    black = np.zeros((round(100 - brake * 100), 10))
    brake_display = np.concatenate((black, white)) * 255

    white = np.ones((round(gaz * 100), 10))
    black = np.zeros((round(100 - gaz * 100), 10))
    gaz_display = np.concatenate((black, white)) * 255

    control_display = np.concatenate((brake_display, gaz_display), axis=1)
    return [steering, gaz, brake]


def compute_steering_speed_gyro_abs(a):
    # Questa funzione è utilizzata dalla black_bar in basso per estrarre i dati di:
    # - velocità,
    # - sterzo,
    # - giroscopio,
    # - quattro sensori ABS.
    right_steering = a[6, 36:46].mean() / 255
    left_steering = a[6, 26:36].mean() / 255
    steering = (right_steering - left_steering + 1.0) / 2

    left_gyro = a[6, 46:60].mean() / 255
    right_gyro = a[6, 60:76].mean() / 255
    gyro = (right_gyro - left_gyro + 1.0) / 2

    speed = a[:, 0][:-2].mean() / 255
    abs1 = a[:, 6][:-2].mean() / 255
    abs2 = a[:, 8][:-2].mean() / 255
    abs3 = a[:, 10][:-2].mean() / 255
    abs4 = a[:, 12][:-2].mean() / 255

    return [steering, speed, gyro, abs1, abs2, abs3, abs4]


# Information reduction from the original image pixels:
#   - Crop the track to 10*10 pixels
#   - 11 valori delle azioni a
state_space_dim = 10 * 10 + 7 + 4


# Run the environment
def run_carRacing(policy, n_episodes=1000, max_t=1500, print_every=100):
    """Run the CarRacing-v0 environment.

    Params
    ======
        n_episodes (int): maximum number of training episodes
        max_t (int): maximum number of timesteps per episode
        print_every (int): how often to print average score (over last 100 episodes)
    """

    scores_deque = deque(maxlen=100)
    scores = []
    trials_to_solve = []

    for i_episode in range(1, n_episodes + 1):
        rewards = []
        state = env.reset()
        if 'reset' in dir(policy):  # Check if the .reset method exists
            policy.reset(state)
        for t in range(max_t):  # Avoid stucked episodes
            action = policy.act(state)
            state, reward, done, info = env.step(action)
            rewards.append(reward)
            if 'memorize' in dir(policy):  # Check if the .memorize method exists
                policy.memorize(state, action, reward, done)
            # Environment must be rendered! If not, all pixels are white...
            env.render()  # (mode='rgb_array')
            if done:
                trials_to_solve.append(t)
                break

        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        if 'update' in dir(policy):  # Check if the .update method exists
            policy.update(state)  # Update the policy
        if i_episode % print_every == 0:
            print('Episode {}\tAverage Score: {:.2f}\tSteps: {:d}'.format(i_episode, np.mean(scores_deque), t))
        if i_episode % 10 == 0:
            print('** model updated **')
            policy.model.save('race-car.h5')
            # policy.save_model()
        if np.mean(scores_deque) >= 900.0:
            print('Episode {}\tAverage Score: {:.2f}\tSteps: {:d}'.format(i_episode, np.mean(scores_deque), t))
            print('** Environment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(max(1, i_episode - 100), np.mean(scores_deque)))
            break

    if np.mean(scores_deque) < 900.0:
        print('** The environment has never been solved!')
        print('   Mean scores on last 100 runs was < 900.0')
    env.close()
    return scores, trials_to_solve


class Policy_DQLearning():
    def __init__(self, state_space_dim=state_space_dim, action_space_dim=11):
        self.action_space_dim = action_space_dim
        self.state_space_dim = state_space_dim

        print('Action Space:', action_space_dim)
        print('State Space:', state_space_dim)
        print("\n")

        self.gamma = 0.99  # 0.95 is slow  # Discount rate
        self.epsilon = 1.0  # 1.0 0.2 1.0<->0.1  # This should be tuned carefuly
        # self.epsilon = 0.5 / np.sqrt(901)
        self.epsilon_decay = 0.995  # 0.995
        # self.epsilon = 0.3
        # self.epsilon_decay = 0.01
        # self.epsilon_decay = 0.05
        self.epsilon_min = 0.01  # 0.0001 0.001

        self.learning_rate = 0.001  # 0.001  # 0
        # se troppo piccolo convergenza lenta / se troppo grande oscillazione e/o divergenza
        self.learning_rate_decay = 0.1  # 0.01 Learning rate decay

        self.episode = 0  # Episode counter
        self.batch_size = 64  # Numero di neuroni per ogni layer

        print('Deep Neural Networks to model the Q Table:')
        if os.path.exists('race-car.h5'):
            print('** Found a local race-car.h5 model.')
            self.model = load_model('race-car.h5')
            print('race-car.h5 model loaded!')
        else:
            self.model = self._build_model(state_space_dim, action_space_dim)
        self.model.summary()
        # self.memory = []
        self.memory = deque(maxlen=100000)  # We can limit the memory size

    '''
    def _build_model(self, state_space_dim, action_space_dim):
        # Neural Net for Deep-Q learning Model
        model = Sequential()
        print('** Build a 2 Fully Connected layers to model the Q-table of this problem **')
        model.add(Activation('linear'))

        # Optimizer:
        # mse = errore quadratico medio come funzione da minimizzare (loss function)
        # rms = RMSprop(lr=0.005)
        # sgd = SGD(lr=0.1, decay=0.0, momentum=0.0, nesterov=False)
        # Adam(lr=0.0005)
        # Adamax(lr=0.001)
        model.add(Dense(action_space_dim, activation='linear'))  # Linear output so we can have range of real-valued outputs
        model.compile(loss='mse', optimizer=Adamax(lr=self.learning_rate, decay=self.learning_rate_decay))
        return model
    '''

    def _build_model(self, state_space_dim, action_space_dim):
        # Neural Net for Deep-Q learning Model
        model = Sequential()
        print('** Build a 2 Fully Connected layers to model the Q-table of this problem **')

        model.add(Dense(512, input_dim=state_space_dim))  # relu  tanh
        model.add(Activation('linear'))
        model.add(Dense(action_space_dim))  # ultimo livello comprendente un numero di neuroni pari al numero di azioni
        model.add(Activation('linear'))
        model.compile(loss='mse', optimizer=Adamax(lr=self.learning_rate, decay=self.learning_rate_decay))
        return model

    def act(self, state):
        # Genera previsioni di uscita per i campioni in ingresso -> Il calcolo viene effettuato in batch
        # - state = un array o una lista di array
        self.qval = self.model.predict(self.current_state, verbose=0)[0]

        if np.random.random() < self.epsilon:  # epsilon-greedy action selection
            # Restituisce un intero casuala da basso 0 a 10
            self.argmax_qval = random.randint(0, 10)
        else:
            # argmax -> Restituisce gli indici dei valori massimi lungo un asse.
            self.argmax_qval = np.argmax(self.qval)
        return convert_argmax_qval_to_env_action(self.argmax_qval)

    def memorize(self, next_state, action, reward, done):
        # Memorize all observables and environment trial results
        a, b, c = transform(next_state)
        next_state = np.concatenate(  # this is 3 + 7*7 size vector.  all scaled in range 0..1
            (np.array([compute_steering_speed_gyro_abs(a)]).reshape(1, -1).flatten(),
             b.reshape(1, -1).flatten(), c), axis=0)
        next_state = np.array(next_state).reshape(1, self.state_space_dim)
        self.memory.append((self.current_state, action, reward, next_state, done))

        # Standard Q-Learning TD(0) Ad ogni istante di tempo di ogni trial si aggiorna la value function:
        next_qval = self.model.predict(next_state, verbose=0)[0]
        G = reward + self.gamma * np.max(next_qval)
        y = self.qval[:]
        y[self.argmax_qval] = G
        self.train(self.current_state, y)
        self.current_state = next_state

    def train(self, state, G):
        # Trains the model for a fixed number of epochs (iterations on a dataset).
        # - Un array o una lista di array
        # - batch_size, numero di campioni per ogni aggiornamento del gradiente.
        self.model.fit(state, np.array(G).reshape(1, 11), epochs=1,
                       verbose=0)  # ADATTA IL MODELLO PER UN ITERAZIONE SUI DATI

    def update(self, state):
        self.episode += 1  # Increment trial counter
        # Train the Q-Network
        # if len(self.memory) > self.batch_size:  # If there are enough trial in memory
        if len(self.memory) % 4:  # Train only every 4th trial
            # if len(self.memory) > 2000:  # We can lazy start to acquire more data before learn on it
            self.replay_to_train(self.batch_size)

        # Update epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def replay_to_train(self, batch_size):
        state_batch, Q_batch = [], []
        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))

        for state, action, reward, next_state, done in minibatch:
            Q_target = self.model.predict(state)[0]  # Use the model to predict the target
            if done:  # Full reward because environment was solved
                Q_target[np.argmax(Q_target)] = reward
            else:  # Discount the reward by gamma because environment was not solved
                Q_target[np.argmax(Q_target)] = reward + self.gamma * np.max(self.model.predict(next_state)[0])
                # Q_target[0][action]

            state_batch.append(state[0])
            Q_batch.append(Q_target)
        self.model.fit(np.array(state_batch), np.array(Q_batch), batch_size=len(state_batch), verbose=0)

    def reset(self, state):
        # Set current_state
        a, b, c = transform(state)
        self.current_state = np.concatenate((np.array([compute_steering_speed_gyro_abs(a)]).reshape(1, -1).flatten(),
                                             b.reshape(1, -1).flatten(), c), axis=0).reshape(1, self.state_space_dim)

    def save_model(self):
        filepath = 'race-car.h5'
        if Path(filepath).exists():
            timestamp = os.stat(filepath).st_ctime
            print('** OLD', filepath, 'exists! Created:', time.ctime(timestamp))
            shutil.copy(filepath, filepath + '_' + str(timestamp))
        self.model.save('race-car.h5')
        print('** Model saved to', filepath, '!')


#  Performance plots
def plot_performance(scores):
    # Plot the policy performance
    fig = plt.figure()
    ax = fig.add_subplot(111)
    x = np.arange(1, len(scores) + 1)
    y = scores
    plt.scatter(x, y, marker='x', c=y)
    fit = np.polyfit(x, y, deg=4)
    p = np.poly1d(fit)
    plt.plot(x, p(x), "r--")
    plt.ylabel('Score')
    plt.xlabel('Episode #')
    plt.title(policy.__class__.__name__ + ' performance on CarRacing-v0')
    plt.show()


def plot_trials_to_solve(trials_to_solve):
    # Plot the policy number of trials to solve the Environment
    fig = plt.figure()
    ax = fig.add_subplot(111)
    plt.hist(trials_to_solve, bins='auto', density=True, facecolor='g', alpha=0.75)
    plt.ylabel('Frequency')
    plt.xlabel('Number of Trial to solve')
    plt.title(policy.__class__.__name__ + ' trials to solve CarRacing-v0')
    plt.show()


policy = Policy_DQLearning()
scores, trials_to_solve = run_carRacing(policy, n_episodes=1000, max_t=1000, print_every=1)

print('** Mean average score:', np.mean(scores))
plot_performance(scores)
plot_trials_to_solve(trials_to_solve)
