import torch


class Storage:

    def __init__(self, steps_per_update, num_of_processes):
        self.steps_per_update = steps_per_update
        self.num_of_processes = num_of_processes
        self.reset_storage()

    def reset_storage(self):
        self.values = torch.zeros(self.steps_per_update, self.num_of_processes, 1)
        self.rewards = torch.zeros(self.steps_per_update, self.num_of_processes, 1)
        self.action_log_probs = torch.zeros(self.steps_per_update, self.num_of_processes, 1)
        self.entropies = torch.zeros(self.steps_per_update, self.num_of_processes)
        self.dones = torch.zeros(self.steps_per_update, self.num_of_processes, 1)

    def add(self, step, values, rewards, action_log_probs, entropies, dones):
        self.values[step] = values
        self.rewards[step] = rewards
        self.action_log_probs[step] = action_log_probs
        self.entropies[step] = entropies
        self.dones[step] = dones

    def compute_expected_rewards(self, last_values, discount_factor):
        expected_rewards = torch.zeros(self.steps_per_update + 1, self.num_of_processes, 1)
        expected_rewards[-1] = last_values
        for step in reversed(range(self.rewards.size(0))):
            expected_rewards[step] = self.rewards[step] + expected_rewards[step + 1] * discount_factor * (1.0 - self.dones[step])
        return expected_rewards[:-1]
